{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Documentation Fine-Tune-SciBERT : documentation for functions and classes. Course Learn how to combine machine learning with software engineering to design, develop, deploy and iterate on production ML applications. credits: https://madewithml.com/ Code: bansal1600/Fine-Tune-SciBERT","title":"Home"},{"location":"#documentation","text":"Fine-Tune-SciBERT : documentation for functions and classes.","title":"Documentation"},{"location":"#course","text":"Learn how to combine machine learning with software engineering to design, develop, deploy and iterate on production ML applications. credits: https://madewithml.com/ Code: bansal1600/Fine-Tune-SciBERT","title":"Course"},{"location":"madewithml/config/","text":"","title":"Config"},{"location":"madewithml/data/","text":"CustomPreprocessor Custom preprocessor class. Source code in madewithml/data.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class CustomPreprocessor : \"\"\"Custom preprocessor class.\"\"\" def __init__ ( self , class_to_index = {}): self . class_to_index = class_to_index or {} # mutable defaults self . index_to_class = { v : k for k , v in self . class_to_index . items ()} def fit ( self , ds ): tags = ds . unique ( column = \"tag\" ) self . class_to_index = { tag : i for i , tag in enumerate ( tags )} self . index_to_class = { v : k for k , v in self . class_to_index . items ()} return self def transform ( self , ds ): return ds . map_batches ( preprocess , fn_kwargs = { \"class_to_index\" : self . class_to_index }, batch_format = \"pandas\" ) clean_text ( text , stopwords = STOPWORDS ) Clean raw text string. Parameters: text ( str ) \u2013 Raw text to clean. stopwords ( List , default: STOPWORDS ) \u2013 list of words to filter out. Defaults to STOPWORDS. Returns: str ( str ) \u2013 cleaned text. madewithml/data.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def clean_text ( text : str , stopwords : List = STOPWORDS ) -> str : \"\"\"Clean raw text string. Args: text (str): Raw text to clean. stopwords (List, optional): list of words to filter out. Defaults to STOPWORDS. Returns: str: cleaned text. \"\"\" # Lower text = text . lower () # Remove stopwords pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \" \" , text ) # Spacing and filters text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # add spacing text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # strip white space at the ends text = re . sub ( r \"http\\S+\" , \"\" , text ) # remove links return text get_tokenizer () Get or initialize the global tokenizer. madewithml/data.py 108 109 110 111 112 113 114 115 def get_tokenizer (): \"\"\"Get or initialize the global tokenizer.\"\"\" global _tokenizer if _tokenizer is None : _tokenizer = BertTokenizer . from_pretrained ( \"allenai/scibert_scivocab_uncased\" , return_dict = False , cache_dir = \"/tmp/huggingface_cache\" # Use local cache ) return _tokenizer load_data ( dataset_loc , num_samples = None ) Load data from source into a Ray Dataset. Parameters: dataset_loc ( str ) \u2013 Location of the dataset. num_samples ( int , default: None ) \u2013 The number of samples to load. Defaults to None. Returns: Dataset ( Dataset ) \u2013 Our dataset represented by a Ray Dataset. madewithml/data.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_data ( dataset_loc : str , num_samples : int = None ) -> Dataset : \"\"\"Load data from source into a Ray Dataset. Args: dataset_loc (str): Location of the dataset. num_samples (int, optional): The number of samples to load. Defaults to None. Returns: Dataset: Our dataset represented by a Ray Dataset. \"\"\" ds = ray . data . read_csv ( dataset_loc ) ds = ds . random_shuffle ( seed = 1234 ) ds = ray . data . from_items ( ds . take ( num_samples )) if num_samples else ds return ds preprocess ( df , class_to_index ) Preprocess the data in our dataframe. Parameters: df ( DataFrame ) \u2013 Raw dataframe to preprocess. class_to_index ( Dict ) \u2013 Mapping of class names to indices. Returns: Dict ( Dict ) \u2013 preprocessed data (ids, masks, targets). madewithml/data.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def preprocess ( df : pd . DataFrame , class_to_index : Dict ) -> Dict : \"\"\"Preprocess the data in our dataframe. Args: df (pd.DataFrame): Raw dataframe to preprocess. class_to_index (Dict): Mapping of class names to indices. Returns: Dict: preprocessed data (ids, masks, targets). \"\"\" df [ \"text\" ] = df . title + \" \" + df . description # feature engineering df [ \"text\" ] = df . text . apply ( clean_text ) # clean text df = df . drop ( columns = [ \"id\" , \"created_on\" , \"title\" , \"description\" ], errors = \"ignore\" ) # clean dataframe df = df [[ \"text\" , \"tag\" ]] # rearrange columns df [ \"tag\" ] = df [ \"tag\" ] . map ( class_to_index ) # label encoding outputs = tokenize ( df ) return outputs stratify_split ( ds , stratify , test_size , shuffle = True , seed = 1234 ) Split a dataset into train and test splits with equal amounts of data points from each class in the column we want to stratify on. Parameters: ds ( Dataset ) \u2013 Input dataset to split. stratify ( str ) \u2013 Name of column to split on. test_size ( float ) \u2013 Proportion of dataset to split for test set. shuffle ( bool , default: True ) \u2013 whether to shuffle the dataset. Defaults to True. seed ( int , default: 1234 ) \u2013 seed for shuffling. Defaults to 1234. Returns: Tuple [ Dataset , Dataset ] \u2013 Tuple[Dataset, Dataset]: the stratified train and test datasets. madewithml/data.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def stratify_split ( ds : Dataset , stratify : str , test_size : float , shuffle : bool = True , seed : int = 1234 , ) -> Tuple [ Dataset , Dataset ]: \"\"\"Split a dataset into train and test splits with equal amounts of data points from each class in the column we want to stratify on. Args: ds (Dataset): Input dataset to split. stratify (str): Name of column to split on. test_size (float): Proportion of dataset to split for test set. shuffle (bool, optional): whether to shuffle the dataset. Defaults to True. seed (int, optional): seed for shuffling. Defaults to 1234. Returns: Tuple[Dataset, Dataset]: the stratified train and test datasets. \"\"\" def _add_split ( df : pd . DataFrame ) -> pd . DataFrame : # pragma: no cover, used in parent function \"\"\"Naively split a dataframe into train and test splits. Add a column specifying whether it's the train or test split.\"\"\" train , test = train_test_split ( df , test_size = test_size , shuffle = shuffle , random_state = seed ) train [ \"_split\" ] = \"train\" test [ \"_split\" ] = \"test\" return pd . concat ([ train , test ]) def _filter_split ( df : pd . DataFrame , split : str ) -> pd . DataFrame : # pragma: no cover, used in parent function \"\"\"Filter by data points that match the split column's value and return the dataframe with the _split column dropped.\"\"\" return df [ df [ \"_split\" ] == split ] . drop ( \"_split\" , axis = 1 ) # Train, test split with stratify grouped = ds . groupby ( stratify ) . map_groups ( _add_split , batch_format = \"pandas\" ) # group by each unique value in the column we want to stratify on train_ds = grouped . map_batches ( _filter_split , fn_kwargs = { \"split\" : \"train\" }, batch_format = \"pandas\" ) # combine test_ds = grouped . map_batches ( _filter_split , fn_kwargs = { \"split\" : \"test\" }, batch_format = \"pandas\" ) # combine # Shuffle each split (required) train_ds = train_ds . random_shuffle ( seed = seed ) test_ds = test_ds . random_shuffle ( seed = seed ) return train_ds , test_ds tokenize ( batch ) Tokenize the text input in our batch using a tokenizer. Parameters: batch ( Dict ) \u2013 batch of data with the text inputs to tokenize. Returns: Dict ( Dict ) \u2013 batch of data with the results of tokenization ( input_ids and attention_mask ) on the text inputs. madewithml/data.py 118 119 120 121 122 123 124 125 126 127 128 129 def tokenize ( batch : Dict ) -> Dict : \"\"\"Tokenize the text input in our batch using a tokenizer. Args: batch (Dict): batch of data with the text inputs to tokenize. Returns: Dict: batch of data with the results of tokenization (`input_ids` and `attention_mask`) on the text inputs. \"\"\" tokenizer = get_tokenizer () encoded_inputs = tokenizer ( batch [ \"text\" ] . tolist (), return_tensors = \"np\" , padding = \"longest\" ) return dict ( ids = encoded_inputs [ \"input_ids\" ], masks = encoded_inputs [ \"attention_mask\" ], targets = np . array ( batch [ \"tag\" ]))","title":"data"},{"location":"madewithml/data/#madewithml.data.CustomPreprocessor","text":"Custom preprocessor class. Source code in madewithml/data.py 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 class CustomPreprocessor : \"\"\"Custom preprocessor class.\"\"\" def __init__ ( self , class_to_index = {}): self . class_to_index = class_to_index or {} # mutable defaults self . index_to_class = { v : k for k , v in self . class_to_index . items ()} def fit ( self , ds ): tags = ds . unique ( column = \"tag\" ) self . class_to_index = { tag : i for i , tag in enumerate ( tags )} self . index_to_class = { v : k for k , v in self . class_to_index . items ()} return self def transform ( self , ds ): return ds . map_batches ( preprocess , fn_kwargs = { \"class_to_index\" : self . class_to_index }, batch_format = \"pandas\" )","title":"CustomPreprocessor"},{"location":"madewithml/data/#madewithml.data.clean_text","text":"Clean raw text string. Parameters: text ( str ) \u2013 Raw text to clean. stopwords ( List , default: STOPWORDS ) \u2013 list of words to filter out. Defaults to STOPWORDS. Returns: str ( str ) \u2013 cleaned text. madewithml/data.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 def clean_text ( text : str , stopwords : List = STOPWORDS ) -> str : \"\"\"Clean raw text string. Args: text (str): Raw text to clean. stopwords (List, optional): list of words to filter out. Defaults to STOPWORDS. Returns: str: cleaned text. \"\"\" # Lower text = text . lower () # Remove stopwords pattern = re . compile ( r \"\\b(\" + r \"|\" . join ( stopwords ) + r \")\\b\\s*\" ) text = pattern . sub ( \" \" , text ) # Spacing and filters text = re . sub ( r \"([! \\\" '#$%&()*\\+,-./:;<=>?@ \\\\ \\[\\]^_`{|}~])\" , r \" \\1 \" , text ) # add spacing text = re . sub ( \"[^A-Za-z0-9]+\" , \" \" , text ) # remove non alphanumeric chars text = re . sub ( \" +\" , \" \" , text ) # remove multiple spaces text = text . strip () # strip white space at the ends text = re . sub ( r \"http\\S+\" , \"\" , text ) # remove links return text","title":"clean_text()"},{"location":"madewithml/data/#madewithml.data.get_tokenizer","text":"Get or initialize the global tokenizer. madewithml/data.py 108 109 110 111 112 113 114 115 def get_tokenizer (): \"\"\"Get or initialize the global tokenizer.\"\"\" global _tokenizer if _tokenizer is None : _tokenizer = BertTokenizer . from_pretrained ( \"allenai/scibert_scivocab_uncased\" , return_dict = False , cache_dir = \"/tmp/huggingface_cache\" # Use local cache ) return _tokenizer","title":"get_tokenizer()"},{"location":"madewithml/data/#madewithml.data.load_data","text":"Load data from source into a Ray Dataset. Parameters: dataset_loc ( str ) \u2013 Location of the dataset. num_samples ( int , default: None ) \u2013 The number of samples to load. Defaults to None. Returns: Dataset ( Dataset ) \u2013 Our dataset represented by a Ray Dataset. madewithml/data.py 14 15 16 17 18 19 20 21 22 23 24 25 26 27 def load_data ( dataset_loc : str , num_samples : int = None ) -> Dataset : \"\"\"Load data from source into a Ray Dataset. Args: dataset_loc (str): Location of the dataset. num_samples (int, optional): The number of samples to load. Defaults to None. Returns: Dataset: Our dataset represented by a Ray Dataset. \"\"\" ds = ray . data . read_csv ( dataset_loc ) ds = ds . random_shuffle ( seed = 1234 ) ds = ray . data . from_items ( ds . take ( num_samples )) if num_samples else ds return ds","title":"load_data()"},{"location":"madewithml/data/#madewithml.data.preprocess","text":"Preprocess the data in our dataframe. Parameters: df ( DataFrame ) \u2013 Raw dataframe to preprocess. class_to_index ( Dict ) \u2013 Mapping of class names to indices. Returns: Dict ( Dict ) \u2013 preprocessed data (ids, masks, targets). madewithml/data.py 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 def preprocess ( df : pd . DataFrame , class_to_index : Dict ) -> Dict : \"\"\"Preprocess the data in our dataframe. Args: df (pd.DataFrame): Raw dataframe to preprocess. class_to_index (Dict): Mapping of class names to indices. Returns: Dict: preprocessed data (ids, masks, targets). \"\"\" df [ \"text\" ] = df . title + \" \" + df . description # feature engineering df [ \"text\" ] = df . text . apply ( clean_text ) # clean text df = df . drop ( columns = [ \"id\" , \"created_on\" , \"title\" , \"description\" ], errors = \"ignore\" ) # clean dataframe df = df [[ \"text\" , \"tag\" ]] # rearrange columns df [ \"tag\" ] = df [ \"tag\" ] . map ( class_to_index ) # label encoding outputs = tokenize ( df ) return outputs","title":"preprocess()"},{"location":"madewithml/data/#madewithml.data.stratify_split","text":"Split a dataset into train and test splits with equal amounts of data points from each class in the column we want to stratify on. Parameters: ds ( Dataset ) \u2013 Input dataset to split. stratify ( str ) \u2013 Name of column to split on. test_size ( float ) \u2013 Proportion of dataset to split for test set. shuffle ( bool , default: True ) \u2013 whether to shuffle the dataset. Defaults to True. seed ( int , default: 1234 ) \u2013 seed for shuffling. Defaults to 1234. Returns: Tuple [ Dataset , Dataset ] \u2013 Tuple[Dataset, Dataset]: the stratified train and test datasets. madewithml/data.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 def stratify_split ( ds : Dataset , stratify : str , test_size : float , shuffle : bool = True , seed : int = 1234 , ) -> Tuple [ Dataset , Dataset ]: \"\"\"Split a dataset into train and test splits with equal amounts of data points from each class in the column we want to stratify on. Args: ds (Dataset): Input dataset to split. stratify (str): Name of column to split on. test_size (float): Proportion of dataset to split for test set. shuffle (bool, optional): whether to shuffle the dataset. Defaults to True. seed (int, optional): seed for shuffling. Defaults to 1234. Returns: Tuple[Dataset, Dataset]: the stratified train and test datasets. \"\"\" def _add_split ( df : pd . DataFrame ) -> pd . DataFrame : # pragma: no cover, used in parent function \"\"\"Naively split a dataframe into train and test splits. Add a column specifying whether it's the train or test split.\"\"\" train , test = train_test_split ( df , test_size = test_size , shuffle = shuffle , random_state = seed ) train [ \"_split\" ] = \"train\" test [ \"_split\" ] = \"test\" return pd . concat ([ train , test ]) def _filter_split ( df : pd . DataFrame , split : str ) -> pd . DataFrame : # pragma: no cover, used in parent function \"\"\"Filter by data points that match the split column's value and return the dataframe with the _split column dropped.\"\"\" return df [ df [ \"_split\" ] == split ] . drop ( \"_split\" , axis = 1 ) # Train, test split with stratify grouped = ds . groupby ( stratify ) . map_groups ( _add_split , batch_format = \"pandas\" ) # group by each unique value in the column we want to stratify on train_ds = grouped . map_batches ( _filter_split , fn_kwargs = { \"split\" : \"train\" }, batch_format = \"pandas\" ) # combine test_ds = grouped . map_batches ( _filter_split , fn_kwargs = { \"split\" : \"test\" }, batch_format = \"pandas\" ) # combine # Shuffle each split (required) train_ds = train_ds . random_shuffle ( seed = seed ) test_ds = test_ds . random_shuffle ( seed = seed ) return train_ds , test_ds","title":"stratify_split()"},{"location":"madewithml/data/#madewithml.data.tokenize","text":"Tokenize the text input in our batch using a tokenizer. Parameters: batch ( Dict ) \u2013 batch of data with the text inputs to tokenize. Returns: Dict ( Dict ) \u2013 batch of data with the results of tokenization ( input_ids and attention_mask ) on the text inputs. madewithml/data.py 118 119 120 121 122 123 124 125 126 127 128 129 def tokenize ( batch : Dict ) -> Dict : \"\"\"Tokenize the text input in our batch using a tokenizer. Args: batch (Dict): batch of data with the text inputs to tokenize. Returns: Dict: batch of data with the results of tokenization (`input_ids` and `attention_mask`) on the text inputs. \"\"\" tokenizer = get_tokenizer () encoded_inputs = tokenizer ( batch [ \"text\" ] . tolist (), return_tensors = \"np\" , padding = \"longest\" ) return dict ( ids = encoded_inputs [ \"input_ids\" ], masks = encoded_inputs [ \"attention_mask\" ], targets = np . array ( batch [ \"tag\" ]))","title":"tokenize()"},{"location":"madewithml/evaluate/","text":"evaluate ( run_id = None , dataset_loc = None , results_fp = None ) Evaluate on the holdout dataset. Parameters: run_id ( str , default: None ) \u2013 id of the specific run to load from. Defaults to None. dataset_loc ( str , default: None ) \u2013 dataset (with labels) to evaluate on. results_fp ( str , default: None ) \u2013 location to save evaluation results to. Defaults to None. Returns: Dict ( Dict ) \u2013 model's performance metrics on the dataset. madewithml/evaluate.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @app . command () def evaluate ( run_id : Annotated [ str , typer . Option ( help = \"id of the specific run to load from\" )] = None , dataset_loc : Annotated [ str , typer . Option ( help = \"dataset (with labels) to evaluate on\" )] = None , results_fp : Annotated [ str , typer . Option ( help = \"location to save evaluation results to\" )] = None , ) -> Dict : # pragma: no cover, eval workload \"\"\"Evaluate on the holdout dataset. Args: run_id (str): id of the specific run to load from. Defaults to None. dataset_loc (str): dataset (with labels) to evaluate on. results_fp (str, optional): location to save evaluation results to. Defaults to None. Returns: Dict: model's performance metrics on the dataset. \"\"\" # Load ds = ray . data . read_csv ( dataset_loc ) best_checkpoint = predict . get_best_checkpoint ( run_id = run_id ) predictor = TorchPredictor . from_checkpoint ( best_checkpoint ) # y_true preprocessor = predictor . get_preprocessor () preprocessed_ds = preprocessor . transform ( ds ) values = preprocessed_ds . select_columns ( cols = [ \"targets\" ]) . take_all () y_true = np . stack ([ item [ \"targets\" ] for item in values ]) # y_pred predictions = preprocessed_ds . map_batches ( predictor ) . take_all () y_pred = np . array ([ d [ \"output\" ] for d in predictions ]) # Metrics metrics = { \"timestamp\" : datetime . datetime . now () . strftime ( \"%B %d , %Y %I:%M:%S %p\" ), \"run_id\" : run_id , \"overall\" : get_overall_metrics ( y_true = y_true , y_pred = y_pred ), \"per_class\" : get_per_class_metrics ( y_true = y_true , y_pred = y_pred , class_to_index = preprocessor . class_to_index ), \"slices\" : get_slice_metrics ( y_true = y_true , y_pred = y_pred , ds = ds ), } logger . info ( json . dumps ( metrics , indent = 2 )) if results_fp : # pragma: no cover, saving results utils . save_dict ( d = metrics , path = results_fp ) return metrics get_overall_metrics ( y_true , y_pred ) Get overall performance metrics. Parameters: y_true ( ndarray ) \u2013 ground truth labels. y_pred ( ndarray ) \u2013 predicted labels. Returns: Dict ( Dict ) \u2013 overall metrics. madewithml/evaluate.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_overall_metrics ( y_true : np . ndarray , y_pred : np . ndarray ) -> Dict : # pragma: no cover, eval workload \"\"\"Get overall performance metrics. Args: y_true (np.ndarray): ground truth labels. y_pred (np.ndarray): predicted labels. Returns: Dict: overall metrics. \"\"\" metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) overall_metrics = { \"precision\" : metrics [ 0 ], \"recall\" : metrics [ 1 ], \"f1\" : metrics [ 2 ], \"num_samples\" : np . float64 ( len ( y_true )), } return overall_metrics get_per_class_metrics ( y_true , y_pred , class_to_index ) Get per class performance metrics. Parameters: y_true ( ndarray ) \u2013 ground truth labels. y_pred ( ndarray ) \u2013 predicted labels. class_to_index ( Dict ) \u2013 dictionary mapping class to index. Returns: Dict ( Dict ) \u2013 per class metrics. madewithml/evaluate.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_per_class_metrics ( y_true : np . ndarray , y_pred : np . ndarray , class_to_index : Dict ) -> Dict : # pragma: no cover, eval workload \"\"\"Get per class performance metrics. Args: y_true (np.ndarray): ground truth labels. y_pred (np.ndarray): predicted labels. class_to_index (Dict): dictionary mapping class to index. Returns: Dict: per class metrics. \"\"\" per_class_metrics = {} metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i , _class in enumerate ( class_to_index ): per_class_metrics [ _class ] = { \"precision\" : metrics [ 0 ][ i ], \"recall\" : metrics [ 1 ][ i ], \"f1\" : metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( metrics [ 3 ][ i ]), } sorted_per_class_metrics = OrderedDict ( sorted ( per_class_metrics . items (), key = lambda tag : tag [ 1 ][ \"f1\" ], reverse = True )) return sorted_per_class_metrics get_slice_metrics ( y_true , y_pred , ds ) Get performance metrics for slices. Parameters: y_true ( ndarray ) \u2013 ground truth labels. y_pred ( ndarray ) \u2013 predicted labels. ds ( Dataset ) \u2013 Ray dataset with labels. Returns: Dict: performance metrics for slices. madewithml/evaluate.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def get_slice_metrics ( y_true : np . ndarray , y_pred : np . ndarray , ds : Dataset ) -> Dict : # pragma: no cover, eval workload \"\"\"Get performance metrics for slices. Args: y_true (np.ndarray): ground truth labels. y_pred (np.ndarray): predicted labels. ds (Dataset): Ray dataset with labels. Returns: Dict: performance metrics for slices. \"\"\" slice_metrics = {} df = ds . to_pandas () df [ \"text\" ] = df [ \"title\" ] + \" \" + df [ \"description\" ] slices = PandasSFApplier ([ nlp_llm , short_text ]) . apply ( df ) for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) slice_metrics [ slice_name ] = {} slice_metrics [ slice_name ][ \"precision\" ] = metrics [ 0 ] slice_metrics [ slice_name ][ \"recall\" ] = metrics [ 1 ] slice_metrics [ slice_name ][ \"f1\" ] = metrics [ 2 ] slice_metrics [ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ]) return slice_metrics nlp_llm ( x ) NLP projects that use LLMs. madewithml/evaluate.py 67 68 69 70 71 72 73 @slicing_function () def nlp_llm ( x ): # pragma: no cover, eval workload \"\"\"NLP projects that use LLMs.\"\"\" nlp_project = \"natural-language-processing\" in x . tag llm_terms = [ \"transformer\" , \"llm\" , \"bert\" ] llm_project = any ( s . lower () in x . text . lower () for s in llm_terms ) return nlp_project and llm_project short_text ( x ) Projects with short titles and descriptions. madewithml/evaluate.py 76 77 78 79 @slicing_function () def short_text ( x ): # pragma: no cover, eval workload \"\"\"Projects with short titles and descriptions.\"\"\" return len ( x . text . split ()) < 8 # less than 8 words","title":"evaluate"},{"location":"madewithml/evaluate/#madewithml.evaluate.evaluate","text":"Evaluate on the holdout dataset. Parameters: run_id ( str , default: None ) \u2013 id of the specific run to load from. Defaults to None. dataset_loc ( str , default: None ) \u2013 dataset (with labels) to evaluate on. results_fp ( str , default: None ) \u2013 location to save evaluation results to. Defaults to None. Returns: Dict ( Dict ) \u2013 model's performance metrics on the dataset. madewithml/evaluate.py 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 @app . command () def evaluate ( run_id : Annotated [ str , typer . Option ( help = \"id of the specific run to load from\" )] = None , dataset_loc : Annotated [ str , typer . Option ( help = \"dataset (with labels) to evaluate on\" )] = None , results_fp : Annotated [ str , typer . Option ( help = \"location to save evaluation results to\" )] = None , ) -> Dict : # pragma: no cover, eval workload \"\"\"Evaluate on the holdout dataset. Args: run_id (str): id of the specific run to load from. Defaults to None. dataset_loc (str): dataset (with labels) to evaluate on. results_fp (str, optional): location to save evaluation results to. Defaults to None. Returns: Dict: model's performance metrics on the dataset. \"\"\" # Load ds = ray . data . read_csv ( dataset_loc ) best_checkpoint = predict . get_best_checkpoint ( run_id = run_id ) predictor = TorchPredictor . from_checkpoint ( best_checkpoint ) # y_true preprocessor = predictor . get_preprocessor () preprocessed_ds = preprocessor . transform ( ds ) values = preprocessed_ds . select_columns ( cols = [ \"targets\" ]) . take_all () y_true = np . stack ([ item [ \"targets\" ] for item in values ]) # y_pred predictions = preprocessed_ds . map_batches ( predictor ) . take_all () y_pred = np . array ([ d [ \"output\" ] for d in predictions ]) # Metrics metrics = { \"timestamp\" : datetime . datetime . now () . strftime ( \"%B %d , %Y %I:%M:%S %p\" ), \"run_id\" : run_id , \"overall\" : get_overall_metrics ( y_true = y_true , y_pred = y_pred ), \"per_class\" : get_per_class_metrics ( y_true = y_true , y_pred = y_pred , class_to_index = preprocessor . class_to_index ), \"slices\" : get_slice_metrics ( y_true = y_true , y_pred = y_pred , ds = ds ), } logger . info ( json . dumps ( metrics , indent = 2 )) if results_fp : # pragma: no cover, saving results utils . save_dict ( d = metrics , path = results_fp ) return metrics","title":"evaluate()"},{"location":"madewithml/evaluate/#madewithml.evaluate.get_overall_metrics","text":"Get overall performance metrics. Parameters: y_true ( ndarray ) \u2013 ground truth labels. y_pred ( ndarray ) \u2013 predicted labels. Returns: Dict ( Dict ) \u2013 overall metrics. madewithml/evaluate.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 def get_overall_metrics ( y_true : np . ndarray , y_pred : np . ndarray ) -> Dict : # pragma: no cover, eval workload \"\"\"Get overall performance metrics. Args: y_true (np.ndarray): ground truth labels. y_pred (np.ndarray): predicted labels. Returns: Dict: overall metrics. \"\"\" metrics = precision_recall_fscore_support ( y_true , y_pred , average = \"weighted\" ) overall_metrics = { \"precision\" : metrics [ 0 ], \"recall\" : metrics [ 1 ], \"f1\" : metrics [ 2 ], \"num_samples\" : np . float64 ( len ( y_true )), } return overall_metrics","title":"get_overall_metrics()"},{"location":"madewithml/evaluate/#madewithml.evaluate.get_per_class_metrics","text":"Get per class performance metrics. Parameters: y_true ( ndarray ) \u2013 ground truth labels. y_pred ( ndarray ) \u2013 predicted labels. class_to_index ( Dict ) \u2013 dictionary mapping class to index. Returns: Dict ( Dict ) \u2013 per class metrics. madewithml/evaluate.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 def get_per_class_metrics ( y_true : np . ndarray , y_pred : np . ndarray , class_to_index : Dict ) -> Dict : # pragma: no cover, eval workload \"\"\"Get per class performance metrics. Args: y_true (np.ndarray): ground truth labels. y_pred (np.ndarray): predicted labels. class_to_index (Dict): dictionary mapping class to index. Returns: Dict: per class metrics. \"\"\" per_class_metrics = {} metrics = precision_recall_fscore_support ( y_true , y_pred , average = None ) for i , _class in enumerate ( class_to_index ): per_class_metrics [ _class ] = { \"precision\" : metrics [ 0 ][ i ], \"recall\" : metrics [ 1 ][ i ], \"f1\" : metrics [ 2 ][ i ], \"num_samples\" : np . float64 ( metrics [ 3 ][ i ]), } sorted_per_class_metrics = OrderedDict ( sorted ( per_class_metrics . items (), key = lambda tag : tag [ 1 ][ \"f1\" ], reverse = True )) return sorted_per_class_metrics","title":"get_per_class_metrics()"},{"location":"madewithml/evaluate/#madewithml.evaluate.get_slice_metrics","text":"Get performance metrics for slices. Parameters: y_true ( ndarray ) \u2013 ground truth labels. y_pred ( ndarray ) \u2013 predicted labels. ds ( Dataset ) \u2013 Ray dataset with labels. Returns: Dict: performance metrics for slices. madewithml/evaluate.py 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 def get_slice_metrics ( y_true : np . ndarray , y_pred : np . ndarray , ds : Dataset ) -> Dict : # pragma: no cover, eval workload \"\"\"Get performance metrics for slices. Args: y_true (np.ndarray): ground truth labels. y_pred (np.ndarray): predicted labels. ds (Dataset): Ray dataset with labels. Returns: Dict: performance metrics for slices. \"\"\" slice_metrics = {} df = ds . to_pandas () df [ \"text\" ] = df [ \"title\" ] + \" \" + df [ \"description\" ] slices = PandasSFApplier ([ nlp_llm , short_text ]) . apply ( df ) for slice_name in slices . dtype . names : mask = slices [ slice_name ] . astype ( bool ) if sum ( mask ): metrics = precision_recall_fscore_support ( y_true [ mask ], y_pred [ mask ], average = \"micro\" ) slice_metrics [ slice_name ] = {} slice_metrics [ slice_name ][ \"precision\" ] = metrics [ 0 ] slice_metrics [ slice_name ][ \"recall\" ] = metrics [ 1 ] slice_metrics [ slice_name ][ \"f1\" ] = metrics [ 2 ] slice_metrics [ slice_name ][ \"num_samples\" ] = len ( y_true [ mask ]) return slice_metrics","title":"get_slice_metrics()"},{"location":"madewithml/evaluate/#madewithml.evaluate.nlp_llm","text":"NLP projects that use LLMs. madewithml/evaluate.py 67 68 69 70 71 72 73 @slicing_function () def nlp_llm ( x ): # pragma: no cover, eval workload \"\"\"NLP projects that use LLMs.\"\"\" nlp_project = \"natural-language-processing\" in x . tag llm_terms = [ \"transformer\" , \"llm\" , \"bert\" ] llm_project = any ( s . lower () in x . text . lower () for s in llm_terms ) return nlp_project and llm_project","title":"nlp_llm()"},{"location":"madewithml/evaluate/#madewithml.evaluate.short_text","text":"Projects with short titles and descriptions. madewithml/evaluate.py 76 77 78 79 @slicing_function () def short_text ( x ): # pragma: no cover, eval workload \"\"\"Projects with short titles and descriptions.\"\"\" return len ( x . text . split ()) < 8 # less than 8 words","title":"short_text()"},{"location":"madewithml/models/","text":"","title":"models"},{"location":"madewithml/predict/","text":"decode ( indices , index_to_class ) Decode indices to labels. Parameters: indices ( Iterable [ Any ] ) \u2013 Iterable (list, array, etc.) with indices. index_to_class ( Dict ) \u2013 mapping between indices and labels. Returns: List ( List ) \u2013 list of labels. madewithml/predict.py 23 24 25 26 27 28 29 30 31 32 33 def decode ( indices : Iterable [ Any ], index_to_class : Dict ) -> List : \"\"\"Decode indices to labels. Args: indices (Iterable[Any]): Iterable (list, array, etc.) with indices. index_to_class (Dict): mapping between indices and labels. Returns: List: list of labels. \"\"\" return [ index_to_class [ index ] for index in indices ] format_prob ( prob , index_to_class ) Format probabilities to a dictionary mapping class label to probability. Parameters: prob ( Iterable ) \u2013 probabilities. index_to_class ( Dict ) \u2013 mapping between indices and labels. Returns: Dict ( Dict ) \u2013 Dictionary mapping class label to probability. madewithml/predict.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def format_prob ( prob : Iterable , index_to_class : Dict ) -> Dict : \"\"\"Format probabilities to a dictionary mapping class label to probability. Args: prob (Iterable): probabilities. index_to_class (Dict): mapping between indices and labels. Returns: Dict: Dictionary mapping class label to probability. \"\"\" d = {} for i , item in enumerate ( prob ): d [ index_to_class [ i ]] = item return d get_best_checkpoint ( run_id ) Get the best checkpoint from a specific run. Parameters: run_id ( str ) \u2013 ID of the run to get the best checkpoint from. Returns: TorchCheckpoint ( TorchCheckpoint ) \u2013 Best checkpoint from the run. madewithml/predict.py 122 123 124 125 126 127 128 129 130 131 132 133 def get_best_checkpoint ( run_id : str ) -> TorchCheckpoint : # pragma: no cover, mlflow logic \"\"\"Get the best checkpoint from a specific run. Args: run_id (str): ID of the run to get the best checkpoint from. Returns: TorchCheckpoint: Best checkpoint from the run. \"\"\" artifact_dir = urlparse ( mlflow . get_run ( run_id ) . info . artifact_uri ) . path # get path from mlflow results = Result . from_path ( artifact_dir ) return results . best_checkpoints [ 0 ][ 0 ] get_best_run_id ( experiment_name = '' , metric = '' , mode = '' ) Get the best run_id from an MLflow experiment. Parameters: experiment_name ( str , default: '' ) \u2013 name of the experiment. metric ( str , default: '' ) \u2013 metric to filter by. mode ( str , default: '' ) \u2013 direction of metric (ASC/DESC). Returns: str ( str ) \u2013 best run id from experiment. madewithml/predict.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @app . command () def get_best_run_id ( experiment_name : str = \"\" , metric : str = \"\" , mode : str = \"\" ) -> str : # pragma: no cover, mlflow logic \"\"\"Get the best run_id from an MLflow experiment. Args: experiment_name (str): name of the experiment. metric (str): metric to filter by. mode (str): direction of metric (ASC/DESC). Returns: str: best run id from experiment. \"\"\" sorted_runs = mlflow . search_runs ( experiment_names = [ experiment_name ], order_by = [ f \"metrics. { metric } { mode } \" ], ) run_id = sorted_runs . iloc [ 0 ] . run_id print ( run_id ) return run_id predict ( run_id = None , title = None , description = None ) Predict the tag for a project given it's title and description. Parameters: run_id ( str , default: None ) \u2013 id of the specific run to load from. Defaults to None. title ( str , default: None ) \u2013 project title. Defaults to \"\". description ( str , default: None ) \u2013 project description. Defaults to \"\". Returns: Dict ( Dict ) \u2013 prediction results for the input data. madewithml/predict.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @app . command () def predict ( run_id : Annotated [ str , typer . Option ( help = \"id of the specific run to load from\" )] = None , title : Annotated [ str , typer . Option ( help = \"project title\" )] = None , description : Annotated [ str , typer . Option ( help = \"project description\" )] = None , ) -> Dict : # pragma: no cover, tested with inference workload \"\"\"Predict the tag for a project given it's title and description. Args: run_id (str): id of the specific run to load from. Defaults to None. title (str, optional): project title. Defaults to \"\". description (str, optional): project description. Defaults to \"\". Returns: Dict: prediction results for the input data. \"\"\" # Load components best_checkpoint = get_best_checkpoint ( run_id = run_id ) predictor = TorchPredictor . from_checkpoint ( best_checkpoint ) # Predict sample_ds = ray . data . from_items ([{ \"title\" : title , \"description\" : description , \"tag\" : \"other\" }]) results = predict_proba ( ds = sample_ds , predictor = predictor ) logger . info ( json . dumps ( results , cls = NumpyEncoder , indent = 2 )) return results predict_proba ( ds , predictor ) Predict tags (with probabilities) for input data from a dataset. Parameters: ds ( Dataset ) \u2013 dataset with input features. predictor ( TorchPredictor ) \u2013 loaded predictor from a checkpoint. Returns: List ( List ) \u2013 list of predicted labels. madewithml/predict.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def predict_proba ( ds : ray . data . dataset . Dataset , predictor : TorchPredictor , ) -> List : # pragma: no cover, tested with inference workload \"\"\"Predict tags (with probabilities) for input data from a dataset. Args: ds (ray.data.dataset.Dataset): dataset with input features. predictor (TorchPredictor): loaded predictor from a checkpoint. Returns: List: list of predicted labels. \"\"\" preprocessor = predictor . get_preprocessor () preprocessed_ds = preprocessor . transform ( ds ) outputs = preprocessed_ds . map_batches ( predictor . predict_proba ) y_prob = np . array ([ d [ \"output\" ] for d in outputs . take_all ()]) results = [] for i , prob in enumerate ( y_prob ): tag = preprocessor . index_to_class [ prob . argmax ()] results . append ({ \"prediction\" : tag , \"probabilities\" : format_prob ( prob , preprocessor . index_to_class )}) return results","title":"predict"},{"location":"madewithml/predict/#madewithml.predict.decode","text":"Decode indices to labels. Parameters: indices ( Iterable [ Any ] ) \u2013 Iterable (list, array, etc.) with indices. index_to_class ( Dict ) \u2013 mapping between indices and labels. Returns: List ( List ) \u2013 list of labels. madewithml/predict.py 23 24 25 26 27 28 29 30 31 32 33 def decode ( indices : Iterable [ Any ], index_to_class : Dict ) -> List : \"\"\"Decode indices to labels. Args: indices (Iterable[Any]): Iterable (list, array, etc.) with indices. index_to_class (Dict): mapping between indices and labels. Returns: List: list of labels. \"\"\" return [ index_to_class [ index ] for index in indices ]","title":"decode()"},{"location":"madewithml/predict/#madewithml.predict.format_prob","text":"Format probabilities to a dictionary mapping class label to probability. Parameters: prob ( Iterable ) \u2013 probabilities. index_to_class ( Dict ) \u2013 mapping between indices and labels. Returns: Dict ( Dict ) \u2013 Dictionary mapping class label to probability. madewithml/predict.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 def format_prob ( prob : Iterable , index_to_class : Dict ) -> Dict : \"\"\"Format probabilities to a dictionary mapping class label to probability. Args: prob (Iterable): probabilities. index_to_class (Dict): mapping between indices and labels. Returns: Dict: Dictionary mapping class label to probability. \"\"\" d = {} for i , item in enumerate ( prob ): d [ index_to_class [ i ]] = item return d","title":"format_prob()"},{"location":"madewithml/predict/#madewithml.predict.get_best_checkpoint","text":"Get the best checkpoint from a specific run. Parameters: run_id ( str ) \u2013 ID of the run to get the best checkpoint from. Returns: TorchCheckpoint ( TorchCheckpoint ) \u2013 Best checkpoint from the run. madewithml/predict.py 122 123 124 125 126 127 128 129 130 131 132 133 def get_best_checkpoint ( run_id : str ) -> TorchCheckpoint : # pragma: no cover, mlflow logic \"\"\"Get the best checkpoint from a specific run. Args: run_id (str): ID of the run to get the best checkpoint from. Returns: TorchCheckpoint: Best checkpoint from the run. \"\"\" artifact_dir = urlparse ( mlflow . get_run ( run_id ) . info . artifact_uri ) . path # get path from mlflow results = Result . from_path ( artifact_dir ) return results . best_checkpoints [ 0 ][ 0 ]","title":"get_best_checkpoint()"},{"location":"madewithml/predict/#madewithml.predict.get_best_run_id","text":"Get the best run_id from an MLflow experiment. Parameters: experiment_name ( str , default: '' ) \u2013 name of the experiment. metric ( str , default: '' ) \u2013 metric to filter by. mode ( str , default: '' ) \u2013 direction of metric (ASC/DESC). Returns: str ( str ) \u2013 best run id from experiment. madewithml/predict.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @app . command () def get_best_run_id ( experiment_name : str = \"\" , metric : str = \"\" , mode : str = \"\" ) -> str : # pragma: no cover, mlflow logic \"\"\"Get the best run_id from an MLflow experiment. Args: experiment_name (str): name of the experiment. metric (str): metric to filter by. mode (str): direction of metric (ASC/DESC). Returns: str: best run id from experiment. \"\"\" sorted_runs = mlflow . search_runs ( experiment_names = [ experiment_name ], order_by = [ f \"metrics. { metric } { mode } \" ], ) run_id = sorted_runs . iloc [ 0 ] . run_id print ( run_id ) return run_id","title":"get_best_run_id()"},{"location":"madewithml/predict/#madewithml.predict.predict","text":"Predict the tag for a project given it's title and description. Parameters: run_id ( str , default: None ) \u2013 id of the specific run to load from. Defaults to None. title ( str , default: None ) \u2013 project title. Defaults to \"\". description ( str , default: None ) \u2013 project description. Defaults to \"\". Returns: Dict ( Dict ) \u2013 prediction results for the input data. madewithml/predict.py 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 @app . command () def predict ( run_id : Annotated [ str , typer . Option ( help = \"id of the specific run to load from\" )] = None , title : Annotated [ str , typer . Option ( help = \"project title\" )] = None , description : Annotated [ str , typer . Option ( help = \"project description\" )] = None , ) -> Dict : # pragma: no cover, tested with inference workload \"\"\"Predict the tag for a project given it's title and description. Args: run_id (str): id of the specific run to load from. Defaults to None. title (str, optional): project title. Defaults to \"\". description (str, optional): project description. Defaults to \"\". Returns: Dict: prediction results for the input data. \"\"\" # Load components best_checkpoint = get_best_checkpoint ( run_id = run_id ) predictor = TorchPredictor . from_checkpoint ( best_checkpoint ) # Predict sample_ds = ray . data . from_items ([{ \"title\" : title , \"description\" : description , \"tag\" : \"other\" }]) results = predict_proba ( ds = sample_ds , predictor = predictor ) logger . info ( json . dumps ( results , cls = NumpyEncoder , indent = 2 )) return results","title":"predict()"},{"location":"madewithml/predict/#madewithml.predict.predict_proba","text":"Predict tags (with probabilities) for input data from a dataset. Parameters: ds ( Dataset ) \u2013 dataset with input features. predictor ( TorchPredictor ) \u2013 loaded predictor from a checkpoint. Returns: List ( List ) \u2013 list of predicted labels. madewithml/predict.py 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def predict_proba ( ds : ray . data . dataset . Dataset , predictor : TorchPredictor , ) -> List : # pragma: no cover, tested with inference workload \"\"\"Predict tags (with probabilities) for input data from a dataset. Args: ds (ray.data.dataset.Dataset): dataset with input features. predictor (TorchPredictor): loaded predictor from a checkpoint. Returns: List: list of predicted labels. \"\"\" preprocessor = predictor . get_preprocessor () preprocessed_ds = preprocessor . transform ( ds ) outputs = preprocessed_ds . map_batches ( predictor . predict_proba ) y_prob = np . array ([ d [ \"output\" ] for d in outputs . take_all ()]) results = [] for i , prob in enumerate ( y_prob ): tag = preprocessor . index_to_class [ prob . argmax ()] results . append ({ \"prediction\" : tag , \"probabilities\" : format_prob ( prob , preprocessor . index_to_class )}) return results","title":"predict_proba()"},{"location":"madewithml/serve/","text":"ModelDeployment Source code in madewithml/serve.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @serve . deployment ( num_replicas = \"1\" , ray_actor_options = { \"num_cpus\" : 8 , \"num_gpus\" : 0 }) @serve . ingress ( app ) class ModelDeployment : def __init__ ( self , run_id : str , threshold : int = 0.9 ): \"\"\"Initialize the model.\"\"\" self . run_id = run_id self . threshold = threshold mlflow . set_tracking_uri ( MLFLOW_TRACKING_URI ) # so workers have access to model registry best_checkpoint = predict . get_best_checkpoint ( run_id = run_id ) self . predictor = predict . TorchPredictor . from_checkpoint ( best_checkpoint ) @app . get ( \"/\" ) def _index ( self ) -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response @app . get ( \"/run_id/\" ) def _run_id ( self ) -> Dict : \"\"\"Get the run ID.\"\"\" return { \"run_id\" : self . run_id } @app . post ( \"/evaluate/\" ) async def _evaluate ( self , request : Request ) -> Dict : data = await request . json () results = evaluate . evaluate ( run_id = self . run_id , dataset_loc = data . get ( \"dataset\" )) return { \"results\" : results } @app . post ( \"/predict/\" ) async def _predict ( self , request : Request ): data = await request . json () sample_ds = ray . data . from_items ([{ \"title\" : data . get ( \"title\" , \"\" ), \"description\" : data . get ( \"description\" , \"\" ), \"tag\" : \"\" }]) results = predict . predict_proba ( ds = sample_ds , predictor = self . predictor ) # Apply custom logic for i , result in enumerate ( results ): pred = result [ \"prediction\" ] prob = result [ \"probabilities\" ] if prob [ pred ] < self . threshold : results [ i ][ \"prediction\" ] = \"other\" return { \"results\" : results } __init__ ( run_id , threshold = 0.9 ) Initialize the model. madewithml/serve.py 25 26 27 28 29 30 31 def __init__ ( self , run_id : str , threshold : int = 0.9 ): \"\"\"Initialize the model.\"\"\" self . run_id = run_id self . threshold = threshold mlflow . set_tracking_uri ( MLFLOW_TRACKING_URI ) # so workers have access to model registry best_checkpoint = predict . get_best_checkpoint ( run_id = run_id ) self . predictor = predict . TorchPredictor . from_checkpoint ( best_checkpoint )","title":"serve"},{"location":"madewithml/serve/#madewithml.serve.ModelDeployment","text":"Source code in madewithml/serve.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 @serve . deployment ( num_replicas = \"1\" , ray_actor_options = { \"num_cpus\" : 8 , \"num_gpus\" : 0 }) @serve . ingress ( app ) class ModelDeployment : def __init__ ( self , run_id : str , threshold : int = 0.9 ): \"\"\"Initialize the model.\"\"\" self . run_id = run_id self . threshold = threshold mlflow . set_tracking_uri ( MLFLOW_TRACKING_URI ) # so workers have access to model registry best_checkpoint = predict . get_best_checkpoint ( run_id = run_id ) self . predictor = predict . TorchPredictor . from_checkpoint ( best_checkpoint ) @app . get ( \"/\" ) def _index ( self ) -> Dict : \"\"\"Health check.\"\"\" response = { \"message\" : HTTPStatus . OK . phrase , \"status-code\" : HTTPStatus . OK , \"data\" : {}, } return response @app . get ( \"/run_id/\" ) def _run_id ( self ) -> Dict : \"\"\"Get the run ID.\"\"\" return { \"run_id\" : self . run_id } @app . post ( \"/evaluate/\" ) async def _evaluate ( self , request : Request ) -> Dict : data = await request . json () results = evaluate . evaluate ( run_id = self . run_id , dataset_loc = data . get ( \"dataset\" )) return { \"results\" : results } @app . post ( \"/predict/\" ) async def _predict ( self , request : Request ): data = await request . json () sample_ds = ray . data . from_items ([{ \"title\" : data . get ( \"title\" , \"\" ), \"description\" : data . get ( \"description\" , \"\" ), \"tag\" : \"\" }]) results = predict . predict_proba ( ds = sample_ds , predictor = self . predictor ) # Apply custom logic for i , result in enumerate ( results ): pred = result [ \"prediction\" ] prob = result [ \"probabilities\" ] if prob [ pred ] < self . threshold : results [ i ][ \"prediction\" ] = \"other\" return { \"results\" : results }","title":"ModelDeployment"},{"location":"madewithml/serve/#madewithml.serve.ModelDeployment.__init__","text":"Initialize the model. madewithml/serve.py 25 26 27 28 29 30 31 def __init__ ( self , run_id : str , threshold : int = 0.9 ): \"\"\"Initialize the model.\"\"\" self . run_id = run_id self . threshold = threshold mlflow . set_tracking_uri ( MLFLOW_TRACKING_URI ) # so workers have access to model registry best_checkpoint = predict . get_best_checkpoint ( run_id = run_id ) self . predictor = predict . TorchPredictor . from_checkpoint ( best_checkpoint )","title":"__init__()"},{"location":"madewithml/train/","text":"eval_step ( ds , batch_size , model , num_classes , loss_fn ) Eval step. Parameters: ds ( Dataset ) \u2013 dataset to iterate batches from. batch_size ( int ) \u2013 size of each batch. model ( Module ) \u2013 model to train. num_classes ( int ) \u2013 number of classes. loss_fn ( _WeightedLoss ) \u2013 loss function to use between labels and predictions. Returns: Tuple [ float , array , array ] \u2013 Tuple[float, np.array, np.array]: cumulative loss, ground truths and predictions. madewithml/train.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def eval_step ( ds : Dataset , batch_size : int , model : nn . Module , num_classes : int , loss_fn : torch . nn . modules . loss . _WeightedLoss ) -> Tuple [ float , np . array , np . array ]: # pragma: no cover, tested via train workload \"\"\"Eval step. Args: ds (Dataset): dataset to iterate batches from. batch_size (int): size of each batch. model (nn.Module): model to train. num_classes (int): number of classes. loss_fn (torch.nn.loss._WeightedLoss): loss function to use between labels and predictions. Returns: Tuple[float, np.array, np.array]: cumulative loss, ground truths and predictions. \"\"\" model . eval () loss = 0.0 y_trues , y_preds = [], [] ds_generator = ds . iter_torch_batches ( batch_size = batch_size , collate_fn = utils . collate_fn ) with torch . inference_mode (): for i , batch in enumerate ( ds_generator ): z = model ( batch ) targets = F . one_hot ( batch [ \"targets\" ], num_classes = num_classes ) . float () # one-hot (for loss_fn) J = loss_fn ( z , targets ) . item () loss += ( J - loss ) / ( i + 1 ) y_trues . extend ( batch [ \"targets\" ] . cpu () . numpy ()) y_preds . extend ( torch . argmax ( z , dim = 1 ) . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_preds ) train_loop_per_worker ( config ) Training loop that each worker will execute. Parameters: config ( dict ) \u2013 arguments to use for training. madewithml/train.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def train_loop_per_worker ( config : dict ) -> None : # pragma: no cover, tested via train workload \"\"\"Training loop that each worker will execute. Args: config (dict): arguments to use for training. \"\"\" # Hyperparameters dropout_p = config [ \"dropout_p\" ] lr = config [ \"lr\" ] lr_factor = config [ \"lr_factor\" ] lr_patience = config [ \"lr_patience\" ] num_epochs = config [ \"num_epochs\" ] batch_size = config [ \"batch_size\" ] num_classes = config [ \"num_classes\" ] # Get datasets utils . set_seeds () train_ds = train . get_dataset_shard ( \"train\" ) val_ds = train . get_dataset_shard ( \"val\" ) # Model llm = BertModel . from_pretrained ( \"allenai/scibert_scivocab_uncased\" , return_dict = False , cache_dir = \"/tmp/huggingface_cache\" # Use local cache to avoid repeated downloads ) model = FinetunedLLM ( llm = llm , dropout_p = dropout_p , embedding_dim = llm . config . hidden_size , num_classes = num_classes ) model = train . torch . prepare_model ( model ) # Training components loss_fn = nn . BCEWithLogitsLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = lr_factor , patience = lr_patience ) # Training num_workers = train . get_context () . get_world_size () batch_size_per_worker = batch_size // num_workers for epoch in range ( num_epochs ): # Step train_loss = train_step ( train_ds , batch_size_per_worker , model , num_classes , loss_fn , optimizer ) val_loss , _ , _ = eval_step ( val_ds , batch_size_per_worker , model , num_classes , loss_fn ) scheduler . step ( val_loss ) # Checkpoint with tempfile . TemporaryDirectory () as dp : if isinstance ( model , DistributedDataParallel ): # cpu model . module . save ( dp = dp ) else : model . save ( dp = dp ) metrics = dict ( epoch = epoch , lr = optimizer . param_groups [ 0 ][ \"lr\" ], train_loss = train_loss , val_loss = val_loss ) checkpoint = Checkpoint . from_directory ( dp ) train . report ( metrics , checkpoint = checkpoint ) train_model ( experiment_name = None , dataset_loc = None , train_loop_config = None , num_workers = 1 , cpu_per_worker = 1 , gpu_per_worker = 0 , num_samples = None , num_epochs = 1 , batch_size = 256 , results_fp = None ) Main train function to train our model as a distributed workload. Parameters: experiment_name ( str , default: None ) \u2013 name of the experiment for this training workload. dataset_loc ( str , default: None ) \u2013 location of the dataset. train_loop_config ( str , default: None ) \u2013 arguments to use for training. num_workers ( int , default: 1 ) \u2013 number of workers to use for training. Defaults to 1. cpu_per_worker ( int , default: 1 ) \u2013 number of CPUs to use per worker. Defaults to 1. gpu_per_worker ( int , default: 0 ) \u2013 number of GPUs to use per worker. Defaults to 0. num_samples ( int , default: None ) \u2013 number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs ( int , default: 1 ) \u2013 number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size ( int , default: 256 ) \u2013 number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp ( str , default: None ) \u2013 filepath to save results to. Defaults to None. Returns: Result \u2013 ray.air.result.Result: training results. madewithml/train.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 @app . command () def train_model ( experiment_name : Annotated [ str , typer . Option ( help = \"name of the experiment for this training workload.\" )] = None , dataset_loc : Annotated [ str , typer . Option ( help = \"location of the dataset.\" )] = None , train_loop_config : Annotated [ str , typer . Option ( help = \"arguments to use for training.\" )] = None , num_workers : Annotated [ int , typer . Option ( help = \"number of workers to use for training.\" )] = 1 , cpu_per_worker : Annotated [ int , typer . Option ( help = \"number of CPUs to use per worker.\" )] = 1 , gpu_per_worker : Annotated [ int , typer . Option ( help = \"number of GPUs to use per worker.\" )] = 0 , num_samples : Annotated [ int , typer . Option ( help = \"number of samples to use from dataset.\" )] = None , num_epochs : Annotated [ int , typer . Option ( help = \"number of epochs to train for.\" )] = 1 , batch_size : Annotated [ int , typer . Option ( help = \"number of samples per batch.\" )] = 256 , results_fp : Annotated [ str , typer . Option ( help = \"filepath to save results to.\" )] = None , ) -> ray . air . result . Result : \"\"\"Main train function to train our model as a distributed workload. Args: experiment_name (str): name of the experiment for this training workload. dataset_loc (str): location of the dataset. train_loop_config (str): arguments to use for training. num_workers (int, optional): number of workers to use for training. Defaults to 1. cpu_per_worker (int, optional): number of CPUs to use per worker. Defaults to 1. gpu_per_worker (int, optional): number of GPUs to use per worker. Defaults to 0. num_samples (int, optional): number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs (int, optional): number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size (int, optional): number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp (str, optional): filepath to save results to. Defaults to None. Returns: ray.air.result.Result: training results. \"\"\" # Set up train_loop_config = json . loads ( train_loop_config ) train_loop_config [ \"num_samples\" ] = num_samples train_loop_config [ \"num_epochs\" ] = num_epochs train_loop_config [ \"batch_size\" ] = batch_size # Scaling config scaling_config = ScalingConfig ( num_workers = num_workers , use_gpu = bool ( gpu_per_worker ), resources_per_worker = { \"CPU\" : cpu_per_worker , \"GPU\" : gpu_per_worker }, ) # Checkpoint config checkpoint_config = CheckpointConfig ( num_to_keep = 1 , checkpoint_score_attribute = \"val_loss\" , checkpoint_score_order = \"min\" , ) # MLflow callback mlflow_callback = MLflowLoggerCallback ( tracking_uri = MLFLOW_TRACKING_URI , experiment_name = experiment_name , save_artifact = True , ) # Run config run_config = RunConfig ( callbacks = [ mlflow_callback ], checkpoint_config = checkpoint_config , storage_path = EFS_DIR , local_dir = EFS_DIR ) # Dataset ds = data . load_data ( dataset_loc = dataset_loc , num_samples = train_loop_config [ \"num_samples\" ]) train_ds , val_ds = data . stratify_split ( ds , stratify = \"tag\" , test_size = 0.2 ) tags = train_ds . unique ( column = \"tag\" ) train_loop_config [ \"num_classes\" ] = len ( tags ) # Dataset config options = ray . data . ExecutionOptions ( preserve_order = True ) dataset_config = DataConfig ( datasets_to_split = [ \"train\" ], execution_options = options ) # Preprocess preprocessor = data . CustomPreprocessor () preprocessor = preprocessor . fit ( train_ds ) train_ds = preprocessor . transform ( train_ds ) val_ds = preprocessor . transform ( val_ds ) train_ds = train_ds . materialize () val_ds = val_ds . materialize () # Trainer trainer = TorchTrainer ( train_loop_per_worker = train_loop_per_worker , train_loop_config = train_loop_config , scaling_config = scaling_config , run_config = run_config , datasets = { \"train\" : train_ds , \"val\" : val_ds }, dataset_config = dataset_config , metadata = { \"class_to_index\" : preprocessor . class_to_index }, ) # Train results = trainer . fit () d = { \"timestamp\" : datetime . datetime . now () . strftime ( \"%B %d , %Y %I:%M:%S %p\" ), \"run_id\" : utils . get_run_id ( experiment_name = experiment_name , trial_id = results . metrics [ \"trial_id\" ]), \"params\" : results . config [ \"train_loop_config\" ], \"metrics\" : utils . dict_to_list ( results . metrics_dataframe . to_dict (), keys = [ \"epoch\" , \"train_loss\" , \"val_loss\" ]), } logger . info ( json . dumps ( d , indent = 2 )) if results_fp : # pragma: no cover, saving results utils . save_dict ( d , results_fp ) return results train_step ( ds , batch_size , model , num_classes , loss_fn , optimizer ) Train step. Parameters: ds ( Dataset ) \u2013 dataset to iterate batches from. batch_size ( int ) \u2013 size of each batch. model ( Module ) \u2013 model to train. num_classes ( int ) \u2013 number of classes. loss_fn ( _WeightedLoss ) \u2013 loss function to use between labels and predictions. optimizer ( Optimizer ) \u2013 optimizer to use for updating the model's weights. Returns: float ( float ) \u2013 cumulative loss for the dataset. madewithml/train.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def train_step ( ds : Dataset , batch_size : int , model : nn . Module , num_classes : int , loss_fn : torch . nn . modules . loss . _WeightedLoss , optimizer : torch . optim . Optimizer , ) -> float : # pragma: no cover, tested via train workload \"\"\"Train step. Args: ds (Dataset): dataset to iterate batches from. batch_size (int): size of each batch. model (nn.Module): model to train. num_classes (int): number of classes. loss_fn (torch.nn.loss._WeightedLoss): loss function to use between labels and predictions. optimizer (torch.optimizer.Optimizer): optimizer to use for updating the model's weights. Returns: float: cumulative loss for the dataset. \"\"\" model . train () loss = 0.0 ds_generator = ds . iter_torch_batches ( batch_size = batch_size , collate_fn = utils . collate_fn ) for i , batch in enumerate ( ds_generator ): optimizer . zero_grad () # reset gradients z = model ( batch ) # forward pass targets = F . one_hot ( batch [ \"targets\" ], num_classes = num_classes ) . float () # one-hot (for loss_fn) J = loss_fn ( z , targets ) # define loss J . backward () # backward pass optimizer . step () # update weights loss += ( J . detach () . item () - loss ) / ( i + 1 ) # cumulative loss return loss","title":"train"},{"location":"madewithml/train/#madewithml.train.eval_step","text":"Eval step. Parameters: ds ( Dataset ) \u2013 dataset to iterate batches from. batch_size ( int ) \u2013 size of each batch. model ( Module ) \u2013 model to train. num_classes ( int ) \u2013 number of classes. loss_fn ( _WeightedLoss ) \u2013 loss function to use between labels and predictions. Returns: Tuple [ float , array , array ] \u2013 Tuple[float, np.array, np.array]: cumulative loss, ground truths and predictions. madewithml/train.py 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 def eval_step ( ds : Dataset , batch_size : int , model : nn . Module , num_classes : int , loss_fn : torch . nn . modules . loss . _WeightedLoss ) -> Tuple [ float , np . array , np . array ]: # pragma: no cover, tested via train workload \"\"\"Eval step. Args: ds (Dataset): dataset to iterate batches from. batch_size (int): size of each batch. model (nn.Module): model to train. num_classes (int): number of classes. loss_fn (torch.nn.loss._WeightedLoss): loss function to use between labels and predictions. Returns: Tuple[float, np.array, np.array]: cumulative loss, ground truths and predictions. \"\"\" model . eval () loss = 0.0 y_trues , y_preds = [], [] ds_generator = ds . iter_torch_batches ( batch_size = batch_size , collate_fn = utils . collate_fn ) with torch . inference_mode (): for i , batch in enumerate ( ds_generator ): z = model ( batch ) targets = F . one_hot ( batch [ \"targets\" ], num_classes = num_classes ) . float () # one-hot (for loss_fn) J = loss_fn ( z , targets ) . item () loss += ( J - loss ) / ( i + 1 ) y_trues . extend ( batch [ \"targets\" ] . cpu () . numpy ()) y_preds . extend ( torch . argmax ( z , dim = 1 ) . cpu () . numpy ()) return loss , np . vstack ( y_trues ), np . vstack ( y_preds )","title":"eval_step()"},{"location":"madewithml/train/#madewithml.train.train_loop_per_worker","text":"Training loop that each worker will execute. Parameters: config ( dict ) \u2013 arguments to use for training. madewithml/train.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def train_loop_per_worker ( config : dict ) -> None : # pragma: no cover, tested via train workload \"\"\"Training loop that each worker will execute. Args: config (dict): arguments to use for training. \"\"\" # Hyperparameters dropout_p = config [ \"dropout_p\" ] lr = config [ \"lr\" ] lr_factor = config [ \"lr_factor\" ] lr_patience = config [ \"lr_patience\" ] num_epochs = config [ \"num_epochs\" ] batch_size = config [ \"batch_size\" ] num_classes = config [ \"num_classes\" ] # Get datasets utils . set_seeds () train_ds = train . get_dataset_shard ( \"train\" ) val_ds = train . get_dataset_shard ( \"val\" ) # Model llm = BertModel . from_pretrained ( \"allenai/scibert_scivocab_uncased\" , return_dict = False , cache_dir = \"/tmp/huggingface_cache\" # Use local cache to avoid repeated downloads ) model = FinetunedLLM ( llm = llm , dropout_p = dropout_p , embedding_dim = llm . config . hidden_size , num_classes = num_classes ) model = train . torch . prepare_model ( model ) # Training components loss_fn = nn . BCEWithLogitsLoss () optimizer = torch . optim . Adam ( model . parameters (), lr = lr ) scheduler = torch . optim . lr_scheduler . ReduceLROnPlateau ( optimizer , mode = \"min\" , factor = lr_factor , patience = lr_patience ) # Training num_workers = train . get_context () . get_world_size () batch_size_per_worker = batch_size // num_workers for epoch in range ( num_epochs ): # Step train_loss = train_step ( train_ds , batch_size_per_worker , model , num_classes , loss_fn , optimizer ) val_loss , _ , _ = eval_step ( val_ds , batch_size_per_worker , model , num_classes , loss_fn ) scheduler . step ( val_loss ) # Checkpoint with tempfile . TemporaryDirectory () as dp : if isinstance ( model , DistributedDataParallel ): # cpu model . module . save ( dp = dp ) else : model . save ( dp = dp ) metrics = dict ( epoch = epoch , lr = optimizer . param_groups [ 0 ][ \"lr\" ], train_loss = train_loss , val_loss = val_loss ) checkpoint = Checkpoint . from_directory ( dp ) train . report ( metrics , checkpoint = checkpoint )","title":"train_loop_per_worker()"},{"location":"madewithml/train/#madewithml.train.train_model","text":"Main train function to train our model as a distributed workload. Parameters: experiment_name ( str , default: None ) \u2013 name of the experiment for this training workload. dataset_loc ( str , default: None ) \u2013 location of the dataset. train_loop_config ( str , default: None ) \u2013 arguments to use for training. num_workers ( int , default: 1 ) \u2013 number of workers to use for training. Defaults to 1. cpu_per_worker ( int , default: 1 ) \u2013 number of CPUs to use per worker. Defaults to 1. gpu_per_worker ( int , default: 0 ) \u2013 number of GPUs to use per worker. Defaults to 0. num_samples ( int , default: None ) \u2013 number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs ( int , default: 1 ) \u2013 number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size ( int , default: 256 ) \u2013 number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp ( str , default: None ) \u2013 filepath to save results to. Defaults to None. Returns: Result \u2013 ray.air.result.Result: training results. madewithml/train.py 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 @app . command () def train_model ( experiment_name : Annotated [ str , typer . Option ( help = \"name of the experiment for this training workload.\" )] = None , dataset_loc : Annotated [ str , typer . Option ( help = \"location of the dataset.\" )] = None , train_loop_config : Annotated [ str , typer . Option ( help = \"arguments to use for training.\" )] = None , num_workers : Annotated [ int , typer . Option ( help = \"number of workers to use for training.\" )] = 1 , cpu_per_worker : Annotated [ int , typer . Option ( help = \"number of CPUs to use per worker.\" )] = 1 , gpu_per_worker : Annotated [ int , typer . Option ( help = \"number of GPUs to use per worker.\" )] = 0 , num_samples : Annotated [ int , typer . Option ( help = \"number of samples to use from dataset.\" )] = None , num_epochs : Annotated [ int , typer . Option ( help = \"number of epochs to train for.\" )] = 1 , batch_size : Annotated [ int , typer . Option ( help = \"number of samples per batch.\" )] = 256 , results_fp : Annotated [ str , typer . Option ( help = \"filepath to save results to.\" )] = None , ) -> ray . air . result . Result : \"\"\"Main train function to train our model as a distributed workload. Args: experiment_name (str): name of the experiment for this training workload. dataset_loc (str): location of the dataset. train_loop_config (str): arguments to use for training. num_workers (int, optional): number of workers to use for training. Defaults to 1. cpu_per_worker (int, optional): number of CPUs to use per worker. Defaults to 1. gpu_per_worker (int, optional): number of GPUs to use per worker. Defaults to 0. num_samples (int, optional): number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs (int, optional): number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size (int, optional): number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp (str, optional): filepath to save results to. Defaults to None. Returns: ray.air.result.Result: training results. \"\"\" # Set up train_loop_config = json . loads ( train_loop_config ) train_loop_config [ \"num_samples\" ] = num_samples train_loop_config [ \"num_epochs\" ] = num_epochs train_loop_config [ \"batch_size\" ] = batch_size # Scaling config scaling_config = ScalingConfig ( num_workers = num_workers , use_gpu = bool ( gpu_per_worker ), resources_per_worker = { \"CPU\" : cpu_per_worker , \"GPU\" : gpu_per_worker }, ) # Checkpoint config checkpoint_config = CheckpointConfig ( num_to_keep = 1 , checkpoint_score_attribute = \"val_loss\" , checkpoint_score_order = \"min\" , ) # MLflow callback mlflow_callback = MLflowLoggerCallback ( tracking_uri = MLFLOW_TRACKING_URI , experiment_name = experiment_name , save_artifact = True , ) # Run config run_config = RunConfig ( callbacks = [ mlflow_callback ], checkpoint_config = checkpoint_config , storage_path = EFS_DIR , local_dir = EFS_DIR ) # Dataset ds = data . load_data ( dataset_loc = dataset_loc , num_samples = train_loop_config [ \"num_samples\" ]) train_ds , val_ds = data . stratify_split ( ds , stratify = \"tag\" , test_size = 0.2 ) tags = train_ds . unique ( column = \"tag\" ) train_loop_config [ \"num_classes\" ] = len ( tags ) # Dataset config options = ray . data . ExecutionOptions ( preserve_order = True ) dataset_config = DataConfig ( datasets_to_split = [ \"train\" ], execution_options = options ) # Preprocess preprocessor = data . CustomPreprocessor () preprocessor = preprocessor . fit ( train_ds ) train_ds = preprocessor . transform ( train_ds ) val_ds = preprocessor . transform ( val_ds ) train_ds = train_ds . materialize () val_ds = val_ds . materialize () # Trainer trainer = TorchTrainer ( train_loop_per_worker = train_loop_per_worker , train_loop_config = train_loop_config , scaling_config = scaling_config , run_config = run_config , datasets = { \"train\" : train_ds , \"val\" : val_ds }, dataset_config = dataset_config , metadata = { \"class_to_index\" : preprocessor . class_to_index }, ) # Train results = trainer . fit () d = { \"timestamp\" : datetime . datetime . now () . strftime ( \"%B %d , %Y %I:%M:%S %p\" ), \"run_id\" : utils . get_run_id ( experiment_name = experiment_name , trial_id = results . metrics [ \"trial_id\" ]), \"params\" : results . config [ \"train_loop_config\" ], \"metrics\" : utils . dict_to_list ( results . metrics_dataframe . to_dict (), keys = [ \"epoch\" , \"train_loss\" , \"val_loss\" ]), } logger . info ( json . dumps ( d , indent = 2 )) if results_fp : # pragma: no cover, saving results utils . save_dict ( d , results_fp ) return results","title":"train_model()"},{"location":"madewithml/train/#madewithml.train.train_step","text":"Train step. Parameters: ds ( Dataset ) \u2013 dataset to iterate batches from. batch_size ( int ) \u2013 size of each batch. model ( Module ) \u2013 model to train. num_classes ( int ) \u2013 number of classes. loss_fn ( _WeightedLoss ) \u2013 loss function to use between labels and predictions. optimizer ( Optimizer ) \u2013 optimizer to use for updating the model's weights. Returns: float ( float ) \u2013 cumulative loss for the dataset. madewithml/train.py 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def train_step ( ds : Dataset , batch_size : int , model : nn . Module , num_classes : int , loss_fn : torch . nn . modules . loss . _WeightedLoss , optimizer : torch . optim . Optimizer , ) -> float : # pragma: no cover, tested via train workload \"\"\"Train step. Args: ds (Dataset): dataset to iterate batches from. batch_size (int): size of each batch. model (nn.Module): model to train. num_classes (int): number of classes. loss_fn (torch.nn.loss._WeightedLoss): loss function to use between labels and predictions. optimizer (torch.optimizer.Optimizer): optimizer to use for updating the model's weights. Returns: float: cumulative loss for the dataset. \"\"\" model . train () loss = 0.0 ds_generator = ds . iter_torch_batches ( batch_size = batch_size , collate_fn = utils . collate_fn ) for i , batch in enumerate ( ds_generator ): optimizer . zero_grad () # reset gradients z = model ( batch ) # forward pass targets = F . one_hot ( batch [ \"targets\" ], num_classes = num_classes ) . float () # one-hot (for loss_fn) J = loss_fn ( z , targets ) # define loss J . backward () # backward pass optimizer . step () # update weights loss += ( J . detach () . item () - loss ) / ( i + 1 ) # cumulative loss return loss","title":"train_step()"},{"location":"madewithml/tune/","text":"tune_models ( experiment_name = None , dataset_loc = None , initial_params = None , num_workers = 1 , cpu_per_worker = 1 , gpu_per_worker = 0 , num_runs = 1 , num_samples = None , num_epochs = 1 , batch_size = 256 , results_fp = None ) Hyperparameter tuning experiment. Parameters: experiment_name ( str , default: None ) \u2013 name of the experiment for this training workload. dataset_loc ( str , default: None ) \u2013 location of the dataset. initial_params ( str , default: None ) \u2013 initial config for the tuning workload. num_workers ( int , default: 1 ) \u2013 number of workers to use for training. Defaults to 1. cpu_per_worker ( int , default: 1 ) \u2013 number of CPUs to use per worker. Defaults to 1. gpu_per_worker ( int , default: 0 ) \u2013 number of GPUs to use per worker. Defaults to 0. num_runs ( int , default: 1 ) \u2013 number of runs in this tuning experiment. Defaults to 1. num_samples ( int , default: None ) \u2013 number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs ( int , default: 1 ) \u2013 number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size ( int , default: 256 ) \u2013 number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp ( str , default: None ) \u2013 filepath to save the tuning results. Defaults to None. Returns: ResultGrid \u2013 ray.tune.result_grid.ResultGrid: results of the tuning experiment. madewithml/tune.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 @app . command () def tune_models ( experiment_name : Annotated [ str , typer . Option ( help = \"name of the experiment for this training workload.\" )] = None , dataset_loc : Annotated [ str , typer . Option ( help = \"location of the dataset.\" )] = None , initial_params : Annotated [ str , typer . Option ( help = \"initial config for the tuning workload.\" )] = None , num_workers : Annotated [ int , typer . Option ( help = \"number of workers to use for training.\" )] = 1 , cpu_per_worker : Annotated [ int , typer . Option ( help = \"number of CPUs to use per worker.\" )] = 1 , gpu_per_worker : Annotated [ int , typer . Option ( help = \"number of GPUs to use per worker.\" )] = 0 , num_runs : Annotated [ int , typer . Option ( help = \"number of runs in this tuning experiment.\" )] = 1 , num_samples : Annotated [ int , typer . Option ( help = \"number of samples to use from dataset.\" )] = None , num_epochs : Annotated [ int , typer . Option ( help = \"number of epochs to train for.\" )] = 1 , batch_size : Annotated [ int , typer . Option ( help = \"number of samples per batch.\" )] = 256 , results_fp : Annotated [ str , typer . Option ( help = \"filepath to save results to.\" )] = None , ) -> ray . tune . result_grid . ResultGrid : \"\"\"Hyperparameter tuning experiment. Args: experiment_name (str): name of the experiment for this training workload. dataset_loc (str): location of the dataset. initial_params (str): initial config for the tuning workload. num_workers (int, optional): number of workers to use for training. Defaults to 1. cpu_per_worker (int, optional): number of CPUs to use per worker. Defaults to 1. gpu_per_worker (int, optional): number of GPUs to use per worker. Defaults to 0. num_runs (int, optional): number of runs in this tuning experiment. Defaults to 1. num_samples (int, optional): number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs (int, optional): number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size (int, optional): number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp (str, optional): filepath to save the tuning results. Defaults to None. Returns: ray.tune.result_grid.ResultGrid: results of the tuning experiment. \"\"\" # Set up utils . set_seeds () train_loop_config = {} train_loop_config [ \"num_samples\" ] = num_samples train_loop_config [ \"num_epochs\" ] = num_epochs train_loop_config [ \"batch_size\" ] = batch_size # Scaling config scaling_config = ScalingConfig ( num_workers = num_workers , use_gpu = bool ( gpu_per_worker ), resources_per_worker = { \"CPU\" : cpu_per_worker , \"GPU\" : gpu_per_worker }, ) # Dataset ds = data . load_data ( dataset_loc = dataset_loc , num_samples = train_loop_config . get ( \"num_samples\" , None )) train_ds , val_ds = data . stratify_split ( ds , stratify = \"tag\" , test_size = 0.2 ) tags = train_ds . unique ( column = \"tag\" ) train_loop_config [ \"num_classes\" ] = len ( tags ) # Dataset config dataset_config = { \"train\" : DatasetConfig ( fit = False , transform = False , randomize_block_order = False ), \"val\" : DatasetConfig ( fit = False , transform = False , randomize_block_order = False ), } # Preprocess preprocessor = data . CustomPreprocessor () preprocessor = preprocessor . fit ( train_ds ) train_ds = preprocessor . transform ( train_ds ) val_ds = preprocessor . transform ( val_ds ) train_ds = train_ds . materialize () val_ds = val_ds . materialize () # Trainer trainer = TorchTrainer ( train_loop_per_worker = train . train_loop_per_worker , train_loop_config = train_loop_config , scaling_config = scaling_config , datasets = { \"train\" : train_ds , \"val\" : val_ds }, dataset_config = dataset_config , metadata = { \"class_to_index\" : preprocessor . class_to_index }, ) # Checkpoint configuration checkpoint_config = CheckpointConfig ( num_to_keep = 1 , checkpoint_score_attribute = \"val_loss\" , checkpoint_score_order = \"min\" , ) # Run configuration mlflow_callback = MLflowLoggerCallback ( tracking_uri = MLFLOW_TRACKING_URI , experiment_name = experiment_name , save_artifact = True , ) run_config = RunConfig ( callbacks = [ mlflow_callback ], checkpoint_config = checkpoint_config , storage_path = EFS_DIR , local_dir = EFS_DIR ) # Hyperparameters to start with initial_params = json . loads ( initial_params ) search_alg = HyperOptSearch ( points_to_evaluate = initial_params ) search_alg = ConcurrencyLimiter ( search_alg , max_concurrent = 2 ) # trade off b/w optimization and search space # Parameter space param_space = { \"train_loop_config\" : { \"dropout_p\" : tune . uniform ( 0.3 , 0.9 ), \"lr\" : tune . loguniform ( 1e-5 , 5e-4 ), \"lr_factor\" : tune . uniform ( 0.1 , 0.9 ), \"lr_patience\" : tune . uniform ( 1 , 10 ), } } # Scheduler scheduler = AsyncHyperBandScheduler ( max_t = train_loop_config [ \"num_epochs\" ], # max epoch (<time_attr>) per trial grace_period = 1 , # min epoch (<time_attr>) per trial ) # Tune config tune_config = tune . TuneConfig ( metric = \"val_loss\" , mode = \"min\" , search_alg = search_alg , scheduler = scheduler , num_samples = num_runs , ) # Tuner tuner = Tuner ( trainable = trainer , run_config = run_config , param_space = param_space , tune_config = tune_config , ) # Tune results = tuner . fit () best_trial = results . get_best_result ( metric = \"val_loss\" , mode = \"min\" ) d = { \"timestamp\" : datetime . datetime . now () . strftime ( \"%B %d , %Y %I:%M:%S %p\" ), \"run_id\" : utils . get_run_id ( experiment_name = experiment_name , trial_id = best_trial . metrics [ \"trial_id\" ]), \"params\" : best_trial . config [ \"train_loop_config\" ], \"metrics\" : utils . dict_to_list ( best_trial . metrics_dataframe . to_dict (), keys = [ \"epoch\" , \"train_loss\" , \"val_loss\" ]), } logger . info ( json . dumps ( d , indent = 2 )) if results_fp : # pragma: no cover, saving results utils . save_dict ( d , results_fp ) return results","title":"tune"},{"location":"madewithml/tune/#madewithml.tune.tune_models","text":"Hyperparameter tuning experiment. Parameters: experiment_name ( str , default: None ) \u2013 name of the experiment for this training workload. dataset_loc ( str , default: None ) \u2013 location of the dataset. initial_params ( str , default: None ) \u2013 initial config for the tuning workload. num_workers ( int , default: 1 ) \u2013 number of workers to use for training. Defaults to 1. cpu_per_worker ( int , default: 1 ) \u2013 number of CPUs to use per worker. Defaults to 1. gpu_per_worker ( int , default: 0 ) \u2013 number of GPUs to use per worker. Defaults to 0. num_runs ( int , default: 1 ) \u2013 number of runs in this tuning experiment. Defaults to 1. num_samples ( int , default: None ) \u2013 number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs ( int , default: 1 ) \u2013 number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size ( int , default: 256 ) \u2013 number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp ( str , default: None ) \u2013 filepath to save the tuning results. Defaults to None. Returns: ResultGrid \u2013 ray.tune.result_grid.ResultGrid: results of the tuning experiment. madewithml/tune.py 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 @app . command () def tune_models ( experiment_name : Annotated [ str , typer . Option ( help = \"name of the experiment for this training workload.\" )] = None , dataset_loc : Annotated [ str , typer . Option ( help = \"location of the dataset.\" )] = None , initial_params : Annotated [ str , typer . Option ( help = \"initial config for the tuning workload.\" )] = None , num_workers : Annotated [ int , typer . Option ( help = \"number of workers to use for training.\" )] = 1 , cpu_per_worker : Annotated [ int , typer . Option ( help = \"number of CPUs to use per worker.\" )] = 1 , gpu_per_worker : Annotated [ int , typer . Option ( help = \"number of GPUs to use per worker.\" )] = 0 , num_runs : Annotated [ int , typer . Option ( help = \"number of runs in this tuning experiment.\" )] = 1 , num_samples : Annotated [ int , typer . Option ( help = \"number of samples to use from dataset.\" )] = None , num_epochs : Annotated [ int , typer . Option ( help = \"number of epochs to train for.\" )] = 1 , batch_size : Annotated [ int , typer . Option ( help = \"number of samples per batch.\" )] = 256 , results_fp : Annotated [ str , typer . Option ( help = \"filepath to save results to.\" )] = None , ) -> ray . tune . result_grid . ResultGrid : \"\"\"Hyperparameter tuning experiment. Args: experiment_name (str): name of the experiment for this training workload. dataset_loc (str): location of the dataset. initial_params (str): initial config for the tuning workload. num_workers (int, optional): number of workers to use for training. Defaults to 1. cpu_per_worker (int, optional): number of CPUs to use per worker. Defaults to 1. gpu_per_worker (int, optional): number of GPUs to use per worker. Defaults to 0. num_runs (int, optional): number of runs in this tuning experiment. Defaults to 1. num_samples (int, optional): number of samples to use from dataset. If this is passed in, it will override the config. Defaults to None. num_epochs (int, optional): number of epochs to train for. If this is passed in, it will override the config. Defaults to None. batch_size (int, optional): number of samples per batch. If this is passed in, it will override the config. Defaults to None. results_fp (str, optional): filepath to save the tuning results. Defaults to None. Returns: ray.tune.result_grid.ResultGrid: results of the tuning experiment. \"\"\" # Set up utils . set_seeds () train_loop_config = {} train_loop_config [ \"num_samples\" ] = num_samples train_loop_config [ \"num_epochs\" ] = num_epochs train_loop_config [ \"batch_size\" ] = batch_size # Scaling config scaling_config = ScalingConfig ( num_workers = num_workers , use_gpu = bool ( gpu_per_worker ), resources_per_worker = { \"CPU\" : cpu_per_worker , \"GPU\" : gpu_per_worker }, ) # Dataset ds = data . load_data ( dataset_loc = dataset_loc , num_samples = train_loop_config . get ( \"num_samples\" , None )) train_ds , val_ds = data . stratify_split ( ds , stratify = \"tag\" , test_size = 0.2 ) tags = train_ds . unique ( column = \"tag\" ) train_loop_config [ \"num_classes\" ] = len ( tags ) # Dataset config dataset_config = { \"train\" : DatasetConfig ( fit = False , transform = False , randomize_block_order = False ), \"val\" : DatasetConfig ( fit = False , transform = False , randomize_block_order = False ), } # Preprocess preprocessor = data . CustomPreprocessor () preprocessor = preprocessor . fit ( train_ds ) train_ds = preprocessor . transform ( train_ds ) val_ds = preprocessor . transform ( val_ds ) train_ds = train_ds . materialize () val_ds = val_ds . materialize () # Trainer trainer = TorchTrainer ( train_loop_per_worker = train . train_loop_per_worker , train_loop_config = train_loop_config , scaling_config = scaling_config , datasets = { \"train\" : train_ds , \"val\" : val_ds }, dataset_config = dataset_config , metadata = { \"class_to_index\" : preprocessor . class_to_index }, ) # Checkpoint configuration checkpoint_config = CheckpointConfig ( num_to_keep = 1 , checkpoint_score_attribute = \"val_loss\" , checkpoint_score_order = \"min\" , ) # Run configuration mlflow_callback = MLflowLoggerCallback ( tracking_uri = MLFLOW_TRACKING_URI , experiment_name = experiment_name , save_artifact = True , ) run_config = RunConfig ( callbacks = [ mlflow_callback ], checkpoint_config = checkpoint_config , storage_path = EFS_DIR , local_dir = EFS_DIR ) # Hyperparameters to start with initial_params = json . loads ( initial_params ) search_alg = HyperOptSearch ( points_to_evaluate = initial_params ) search_alg = ConcurrencyLimiter ( search_alg , max_concurrent = 2 ) # trade off b/w optimization and search space # Parameter space param_space = { \"train_loop_config\" : { \"dropout_p\" : tune . uniform ( 0.3 , 0.9 ), \"lr\" : tune . loguniform ( 1e-5 , 5e-4 ), \"lr_factor\" : tune . uniform ( 0.1 , 0.9 ), \"lr_patience\" : tune . uniform ( 1 , 10 ), } } # Scheduler scheduler = AsyncHyperBandScheduler ( max_t = train_loop_config [ \"num_epochs\" ], # max epoch (<time_attr>) per trial grace_period = 1 , # min epoch (<time_attr>) per trial ) # Tune config tune_config = tune . TuneConfig ( metric = \"val_loss\" , mode = \"min\" , search_alg = search_alg , scheduler = scheduler , num_samples = num_runs , ) # Tuner tuner = Tuner ( trainable = trainer , run_config = run_config , param_space = param_space , tune_config = tune_config , ) # Tune results = tuner . fit () best_trial = results . get_best_result ( metric = \"val_loss\" , mode = \"min\" ) d = { \"timestamp\" : datetime . datetime . now () . strftime ( \"%B %d , %Y %I:%M:%S %p\" ), \"run_id\" : utils . get_run_id ( experiment_name = experiment_name , trial_id = best_trial . metrics [ \"trial_id\" ]), \"params\" : best_trial . config [ \"train_loop_config\" ], \"metrics\" : utils . dict_to_list ( best_trial . metrics_dataframe . to_dict (), keys = [ \"epoch\" , \"train_loss\" , \"val_loss\" ]), } logger . info ( json . dumps ( d , indent = 2 )) if results_fp : # pragma: no cover, saving results utils . save_dict ( d , results_fp ) return results","title":"tune_models()"},{"location":"madewithml/utils/","text":"collate_fn ( batch ) Convert a batch of numpy arrays to tensors (with appropriate padding). Parameters: batch ( Dict [ str , ndarray ] ) \u2013 input batch as a dictionary of numpy arrays. Returns: Dict [ str , Tensor ] \u2013 Dict[str, torch.Tensor]: output batch as a dictionary of tensors. madewithml/utils.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def collate_fn ( batch : Dict [ str , np . ndarray ]) -> Dict [ str , torch . Tensor ]: # pragma: no cover, air internal \"\"\"Convert a batch of numpy arrays to tensors (with appropriate padding). Args: batch (Dict[str, np.ndarray]): input batch as a dictionary of numpy arrays. Returns: Dict[str, torch.Tensor]: output batch as a dictionary of tensors. \"\"\" batch [ \"ids\" ] = pad_array ( batch [ \"ids\" ]) batch [ \"masks\" ] = pad_array ( batch [ \"masks\" ]) dtypes = { \"ids\" : torch . int32 , \"masks\" : torch . int32 , \"targets\" : torch . int64 } tensor_batch = {} for key , array in batch . items (): tensor_batch [ key ] = torch . as_tensor ( array , dtype = dtypes [ key ], device = get_device ()) return tensor_batch dict_to_list ( data , keys ) Convert a dictionary to a list of dictionaries. Parameters: data ( Dict ) \u2013 input dictionary. keys ( List [ str ] ) \u2013 keys to include in the output list of dictionaries. Returns: List [ Dict [ str , Any ]] \u2013 List[Dict[str, Any]]: output list of dictionaries. madewithml/utils.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def dict_to_list ( data : Dict , keys : List [ str ]) -> List [ Dict [ str , Any ]]: \"\"\"Convert a dictionary to a list of dictionaries. Args: data (Dict): input dictionary. keys (List[str]): keys to include in the output list of dictionaries. Returns: List[Dict[str, Any]]: output list of dictionaries. \"\"\" list_of_dicts = [] for i in range ( len ( data [ keys [ 0 ]])): new_dict = { key : data [ key ][ i ] for key in keys } list_of_dicts . append ( new_dict ) return list_of_dicts get_run_id ( experiment_name , trial_id ) Get the MLflow run ID for a specific Ray trial ID. Parameters: experiment_name ( str ) \u2013 name of the experiment. trial_id ( str ) \u2013 id of the trial. Returns: str ( str ) \u2013 run id of the trial. madewithml/utils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 def get_run_id ( experiment_name : str , trial_id : str ) -> str : # pragma: no cover, mlflow functionality \"\"\"Get the MLflow run ID for a specific Ray trial ID. Args: experiment_name (str): name of the experiment. trial_id (str): id of the trial. Returns: str: run id of the trial. \"\"\" trial_name = f \"TorchTrainer_ { trial_id } \" run = mlflow . search_runs ( experiment_names = [ experiment_name ], filter_string = f \"tags.trial_name = ' { trial_name } '\" ) . iloc [ 0 ] return run . run_id load_dict ( path ) Load a dictionary from a JSON's filepath. Parameters: path ( str ) \u2013 location of file. Returns: Dict ( Dict ) \u2013 loaded JSON data. madewithml/utils.py 27 28 29 30 31 32 33 34 35 36 37 38 def load_dict ( path : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: path (str): location of file. Returns: Dict: loaded JSON data. \"\"\" with open ( path ) as fp : d = json . load ( fp ) return d pad_array ( arr , dtype = np . int32 ) Pad an 2D array with zeros until all rows in the 2D array are of the same length as a the longest row in the 2D array. Parameters: arr ( array ) \u2013 input array Returns: ndarray \u2013 np.array: zero padded array madewithml/utils.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def pad_array ( arr : np . ndarray , dtype = np . int32 ) -> np . ndarray : \"\"\"Pad an 2D array with zeros until all rows in the 2D array are of the same length as a the longest row in the 2D array. Args: arr (np.array): input array Returns: np.array: zero padded array \"\"\" max_len = max ( len ( row ) for row in arr ) padded_arr = np . zeros (( arr . shape [ 0 ], max_len ), dtype = dtype ) for i , row in enumerate ( arr ): padded_arr [ i ][: len ( row )] = row return padded_arr save_dict ( d , path , cls = None , sortkeys = False ) Save a dictionary to a specific location. Parameters: d ( Dict ) \u2013 data to save. path ( str ) \u2013 location of where to save the data. cls ( optional , default: None ) \u2013 encoder to use on dict data. Defaults to None. sortkeys ( bool , default: False ) \u2013 whether to sort keys alphabetically. Defaults to False. madewithml/utils.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def save_dict ( d : Dict , path : str , cls : Any = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Args: d (Dict): data to save. path (str): location of where to save the data. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): whether to sort keys alphabetically. Defaults to False. \"\"\" directory = os . path . dirname ( path ) if directory and not os . path . exists ( directory ): # pragma: no cover os . makedirs ( directory ) with open ( path , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys ) fp . write ( \" \\n \" ) set_seeds ( seed = 42 ) Set seeds for reproducibility. madewithml/utils.py 16 17 18 19 20 21 22 23 24 def set_seeds ( seed : int = 42 ): \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) eval ( \"setattr(torch.backends.cudnn, 'deterministic', True)\" ) eval ( \"setattr(torch.backends.cudnn, 'benchmark', False)\" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed )","title":"utils"},{"location":"madewithml/utils/#madewithml.utils.collate_fn","text":"Convert a batch of numpy arrays to tensors (with appropriate padding). Parameters: batch ( Dict [ str , ndarray ] ) \u2013 input batch as a dictionary of numpy arrays. Returns: Dict [ str , Tensor ] \u2013 Dict[str, torch.Tensor]: output batch as a dictionary of tensors. madewithml/utils.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def collate_fn ( batch : Dict [ str , np . ndarray ]) -> Dict [ str , torch . Tensor ]: # pragma: no cover, air internal \"\"\"Convert a batch of numpy arrays to tensors (with appropriate padding). Args: batch (Dict[str, np.ndarray]): input batch as a dictionary of numpy arrays. Returns: Dict[str, torch.Tensor]: output batch as a dictionary of tensors. \"\"\" batch [ \"ids\" ] = pad_array ( batch [ \"ids\" ]) batch [ \"masks\" ] = pad_array ( batch [ \"masks\" ]) dtypes = { \"ids\" : torch . int32 , \"masks\" : torch . int32 , \"targets\" : torch . int64 } tensor_batch = {} for key , array in batch . items (): tensor_batch [ key ] = torch . as_tensor ( array , dtype = dtypes [ key ], device = get_device ()) return tensor_batch","title":"collate_fn()"},{"location":"madewithml/utils/#madewithml.utils.dict_to_list","text":"Convert a dictionary to a list of dictionaries. Parameters: data ( Dict ) \u2013 input dictionary. keys ( List [ str ] ) \u2013 keys to include in the output list of dictionaries. Returns: List [ Dict [ str , Any ]] \u2013 List[Dict[str, Any]]: output list of dictionaries. madewithml/utils.py 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 def dict_to_list ( data : Dict , keys : List [ str ]) -> List [ Dict [ str , Any ]]: \"\"\"Convert a dictionary to a list of dictionaries. Args: data (Dict): input dictionary. keys (List[str]): keys to include in the output list of dictionaries. Returns: List[Dict[str, Any]]: output list of dictionaries. \"\"\" list_of_dicts = [] for i in range ( len ( data [ keys [ 0 ]])): new_dict = { key : data [ key ][ i ] for key in keys } list_of_dicts . append ( new_dict ) return list_of_dicts","title":"dict_to_list()"},{"location":"madewithml/utils/#madewithml.utils.get_run_id","text":"Get the MLflow run ID for a specific Ray trial ID. Parameters: experiment_name ( str ) \u2013 name of the experiment. trial_id ( str ) \u2013 id of the trial. Returns: str ( str ) \u2013 run id of the trial. madewithml/utils.py 94 95 96 97 98 99 100 101 102 103 104 105 106 def get_run_id ( experiment_name : str , trial_id : str ) -> str : # pragma: no cover, mlflow functionality \"\"\"Get the MLflow run ID for a specific Ray trial ID. Args: experiment_name (str): name of the experiment. trial_id (str): id of the trial. Returns: str: run id of the trial. \"\"\" trial_name = f \"TorchTrainer_ { trial_id } \" run = mlflow . search_runs ( experiment_names = [ experiment_name ], filter_string = f \"tags.trial_name = ' { trial_name } '\" ) . iloc [ 0 ] return run . run_id","title":"get_run_id()"},{"location":"madewithml/utils/#madewithml.utils.load_dict","text":"Load a dictionary from a JSON's filepath. Parameters: path ( str ) \u2013 location of file. Returns: Dict ( Dict ) \u2013 loaded JSON data. madewithml/utils.py 27 28 29 30 31 32 33 34 35 36 37 38 def load_dict ( path : str ) -> Dict : \"\"\"Load a dictionary from a JSON's filepath. Args: path (str): location of file. Returns: Dict: loaded JSON data. \"\"\" with open ( path ) as fp : d = json . load ( fp ) return d","title":"load_dict()"},{"location":"madewithml/utils/#madewithml.utils.pad_array","text":"Pad an 2D array with zeros until all rows in the 2D array are of the same length as a the longest row in the 2D array. Parameters: arr ( array ) \u2013 input array Returns: ndarray \u2013 np.array: zero padded array madewithml/utils.py 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 def pad_array ( arr : np . ndarray , dtype = np . int32 ) -> np . ndarray : \"\"\"Pad an 2D array with zeros until all rows in the 2D array are of the same length as a the longest row in the 2D array. Args: arr (np.array): input array Returns: np.array: zero padded array \"\"\" max_len = max ( len ( row ) for row in arr ) padded_arr = np . zeros (( arr . shape [ 0 ], max_len ), dtype = dtype ) for i , row in enumerate ( arr ): padded_arr [ i ][: len ( row )] = row return padded_arr","title":"pad_array()"},{"location":"madewithml/utils/#madewithml.utils.save_dict","text":"Save a dictionary to a specific location. Parameters: d ( Dict ) \u2013 data to save. path ( str ) \u2013 location of where to save the data. cls ( optional , default: None ) \u2013 encoder to use on dict data. Defaults to None. sortkeys ( bool , default: False ) \u2013 whether to sort keys alphabetically. Defaults to False. madewithml/utils.py 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 def save_dict ( d : Dict , path : str , cls : Any = None , sortkeys : bool = False ) -> None : \"\"\"Save a dictionary to a specific location. Args: d (Dict): data to save. path (str): location of where to save the data. cls (optional): encoder to use on dict data. Defaults to None. sortkeys (bool, optional): whether to sort keys alphabetically. Defaults to False. \"\"\" directory = os . path . dirname ( path ) if directory and not os . path . exists ( directory ): # pragma: no cover os . makedirs ( directory ) with open ( path , \"w\" ) as fp : json . dump ( d , indent = 2 , fp = fp , cls = cls , sort_keys = sortkeys ) fp . write ( \" \\n \" )","title":"save_dict()"},{"location":"madewithml/utils/#madewithml.utils.set_seeds","text":"Set seeds for reproducibility. madewithml/utils.py 16 17 18 19 20 21 22 23 24 def set_seeds ( seed : int = 42 ): \"\"\"Set seeds for reproducibility.\"\"\" np . random . seed ( seed ) random . seed ( seed ) torch . manual_seed ( seed ) torch . cuda . manual_seed ( seed ) eval ( \"setattr(torch.backends.cudnn, 'deterministic', True)\" ) eval ( \"setattr(torch.backends.cudnn, 'benchmark', False)\" ) os . environ [ \"PYTHONHASHSEED\" ] = str ( seed )","title":"set_seeds()"}]}